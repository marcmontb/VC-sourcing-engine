{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNU99u+PEchvRpwqW9TDeuF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marcmontb/VC-sourcing-engine/blob/main/AI_scraping_Streamlit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IP39ogNMOjTl",
        "outputId": "2fe1fae4-86ea-494e-a47a-9f3d4baf4773"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.43.2-py2.py3-none-any.whl.metadata (8.9 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.3)\n",
            "Collecting duckduckgo_search\n",
            "  Downloading duckduckgo_search-7.5.1-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.1.0)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.3-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.1.8)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.25.6)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (9.0.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.12.2)\n",
            "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.6)\n",
            "Collecting primp>=0.14.0 (from duckduckgo_search)\n",
            "  Downloading primp-0.14.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: lxml>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from duckduckgo_search) (5.3.1)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.29.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.23.1)\n",
            "Downloading streamlit-1.43.2-py2.py3-none-any.whl (9.7 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading duckduckgo_search-7.5.1-py3-none-any.whl (20 kB)\n",
            "Downloading pyngrok-7.2.3-py3-none-any.whl (23 kB)\n",
            "Downloading primp-0.14.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: watchdog, pyngrok, primp, pydeck, duckduckgo_search, streamlit\n",
            "Successfully installed duckduckgo_search-7.5.1 primp-0.14.0 pydeck-0.9.1 pyngrok-7.2.3 streamlit-1.43.2 watchdog-6.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install streamlit pandas numpy requests matplotlib seaborn beautifulsoup4 duckduckgo_search pillow pyngrok"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1iJlAXTbOwzz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "# Paste the entire Streamlit app code here (from the artifact I provided)\n",
        "\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import time\n",
        "import re\n",
        "import os\n",
        "import io\n",
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "from PIL import Image\n",
        "from datetime import datetime\n",
        "\n",
        "# Set page configuration\n",
        "st.set_page_config(\n",
        "    page_title=\"CRM Data Enrichment Tool\",\n",
        "    page_icon=\"üìä\",\n",
        "    layout=\"wide\",\n",
        "    initial_sidebar_state=\"expanded\"\n",
        ")\n",
        "\n",
        "# Application title and description\n",
        "st.title(\"CRM Data Enrichment Tool\")\n",
        "st.subheader(\"Inspired by Coherent - Online data made easy\")\n",
        "\n",
        "# Sidebar\n",
        "with st.sidebar:\n",
        "    st.header(\"Settings\")\n",
        "\n",
        "    # API Key settings (optional)\n",
        "    st.subheader(\"API Settings (Optional)\")\n",
        "    use_api_keys = st.checkbox(\"Use API Keys\", value=False)\n",
        "\n",
        "    if use_api_keys:\n",
        "        openai_api_key = st.text_input(\"OpenAI API Key\", type=\"password\")\n",
        "        serper_api_key = st.text_input(\"Serper API Key\", type=\"password\")\n",
        "\n",
        "        if openai_api_key:\n",
        "            os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
        "        if serper_api_key:\n",
        "            os.environ[\"SERPER_API_KEY\"] = serper_api_key\n",
        "\n",
        "    # Search settings\n",
        "    st.subheader(\"Search Settings\")\n",
        "    search_delay = st.slider(\"Delay between searches (seconds)\", 1, 10, 2)\n",
        "    max_search_results = st.slider(\"Max search results per company\", 1, 10, 3)\n",
        "\n",
        "    # About section\n",
        "    st.markdown(\"---\")\n",
        "    st.markdown(\"### About\")\n",
        "    st.markdown(\"This tool helps enrich your CRM data by researching companies online and extracting useful information.\")\n",
        "    st.markdown(\"Upload your CSV with company data, customize your settings, and get enhanced insights.\")\n",
        "\n",
        "# Main functions (adapted from the original script)\n",
        "# Web Research Functions\n",
        "@st.cache_data(ttl=3600)  # Cache results for 1 hour\n",
        "def search_company(company_name, domain=None):\n",
        "    \"\"\"Search for company information online\"\"\"\n",
        "    search_query = company_name\n",
        "    if domain:\n",
        "        search_query += f\" {domain}\"\n",
        "\n",
        "    results = []\n",
        "\n",
        "    # Try to import and use duckduckgo_search\n",
        "    try:\n",
        "        from duckduckgo_search import DDGS\n",
        "        ddgs = DDGS()\n",
        "\n",
        "        # First, try to get some results from DuckDuckGo\n",
        "        search_results = ddgs.text(search_query, max_results=max_search_results)\n",
        "        results = list(search_results)\n",
        "\n",
        "        # If we have a domain, also search specifically on that domain\n",
        "        if domain and domain.startswith(('http://', 'https://')):\n",
        "            try:\n",
        "                site_domain = domain.split('//')[1].split('/')[0]\n",
        "                site_results = ddgs.text(f\"site:{site_domain}\", max_results=2)\n",
        "                results.extend(list(site_results))\n",
        "            except:\n",
        "                pass\n",
        "    except Exception as e:\n",
        "        st.warning(f\"Search error: {e}\")\n",
        "        # Fallback to a simple message if search fails\n",
        "        results = [{\"body\": f\"Information about {company_name}\", \"href\": domain if domain else \"\"}]\n",
        "\n",
        "    return results\n",
        "\n",
        "@st.cache_data(ttl=3600)  # Cache results for 1 hour\n",
        "def extract_website_info(url):\n",
        "    \"\"\"Extract text information from a website\"\"\"\n",
        "    try:\n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "        }\n",
        "        response = requests.get(url, headers=headers, timeout=10)\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            # Remove scripts, styles, and other non-content tags\n",
        "            for script in soup([\"script\", \"style\", \"meta\", \"noscript\", \"header\", \"footer\"]):\n",
        "                script.extract()\n",
        "\n",
        "            # Extract title\n",
        "            title = soup.title.string if soup.title else \"\"\n",
        "\n",
        "            # Extract meta description\n",
        "            meta_desc = \"\"\n",
        "            meta_tag = soup.find(\"meta\", attrs={\"name\": \"description\"})\n",
        "            if meta_tag and \"content\" in meta_tag.attrs:\n",
        "                meta_desc = meta_tag[\"content\"]\n",
        "\n",
        "            # Extract main text content\n",
        "            text = soup.get_text(separator=' ', strip=True)\n",
        "            text = re.sub(r'\\s+', ' ', text)  # Normalize whitespace\n",
        "\n",
        "            # Extract potential \"About\" section\n",
        "            about_section = \"\"\n",
        "            about_headers = soup.find_all([\"h1\", \"h2\", \"h3\"], string=re.compile(r'about|company|who we are', re.I))\n",
        "            for header in about_headers:\n",
        "                section = []\n",
        "                for sibling in header.find_next_siblings():\n",
        "                    if sibling.name in [\"h1\", \"h2\", \"h3\"]:\n",
        "                        break\n",
        "                    section.append(sibling.get_text(strip=True))\n",
        "                about_section += \" \".join(section)\n",
        "\n",
        "            return {\n",
        "                \"title\": title,\n",
        "                \"meta_description\": meta_desc,\n",
        "                \"about_section\": about_section,\n",
        "                \"full_text\": text[:5000],  # Limit to first 5000 chars\n",
        "                \"url\": url\n",
        "            }\n",
        "        else:\n",
        "            return {\"error\": f\"Failed to fetch {url}: HTTP {response.status_code}\"}\n",
        "    except Exception as e:\n",
        "        return {\"error\": f\"Error processing {url}: {str(e)}\"}\n",
        "\n",
        "# Industry Classification\n",
        "def classify_industry(company_text):\n",
        "    \"\"\"Classify company industry based on text description\"\"\"\n",
        "\n",
        "    # Dictionary of industry keywords\n",
        "    industry_keywords = {\n",
        "        \"TravelTech\": [\"travel\", \"tourism\", \"booking\", \"flight\", \"hotel\", \"vacation\", \"destination\"],\n",
        "        \"HRTech\": [\"recruitment\", \"talent\", \"hiring\", \"hr \", \"human resources\", \"workforce\", \"staffing\"],\n",
        "        \"FinTech\": [\"finance\", \"banking\", \"insurance\", \"payment\", \"financial\", \"loan\", \"invest\"],\n",
        "        \"HealthTech\": [\"health\", \"medical\", \"healthcare\", \"patient\", \"doctor\", \"hospital\", \"clinic\"],\n",
        "        \"EdTech\": [\"education\", \"learning\", \"teaching\", \"school\", \"student\", \"academic\", \"course\"],\n",
        "        \"ECommerce\": [\"ecommerce\", \"online shop\", \"online store\", \"shopping\", \"retail\", \"marketplace\"],\n",
        "        \"SaaS\": [\"software as a service\", \"saas\", \"subscription software\", \"cloud service\"],\n",
        "        \"Manufacturing\": [\"manufacturing\", \"factory\", \"production\", \"industrial\", \"equipment\"],\n",
        "        \"IT Services\": [\"it services\", \"consulting\", \"system integration\", \"tech support\"],\n",
        "        \"AgTech\": [\"agriculture\", \"farming\", \"crop\", \"livestock\", \"agri-tech\"]\n",
        "    }\n",
        "\n",
        "    # Count matches for each industry\n",
        "    matches = {}\n",
        "    company_text = company_text.lower()\n",
        "\n",
        "    for industry, keywords in industry_keywords.items():\n",
        "        count = sum(1 for keyword in keywords if keyword.lower() in company_text)\n",
        "        if count > 0:\n",
        "            matches[industry] = count\n",
        "\n",
        "    # Return the industry with the most keyword matches\n",
        "    if matches:\n",
        "        return max(matches.items(), key=lambda x: x[1])[0]\n",
        "    else:\n",
        "        return \"Unknown\"\n",
        "\n",
        "# Helper functions for specific indicators\n",
        "def has_sales_jobs(company_text):\n",
        "    \"\"\"Detect if company has sales job openings\"\"\"\n",
        "    sales_keywords = [\n",
        "        \"sales representative\", \"sales manager\", \"sales executive\", \"sales job\",\n",
        "        \"hiring sales\", \"sales position\", \"sales opportunity\", \"sales career\"\n",
        "    ]\n",
        "    return any(keyword in company_text.lower() for keyword in sales_keywords)\n",
        "\n",
        "def has_hr_team(company_text):\n",
        "    \"\"\"Detect if company has an HR team\"\"\"\n",
        "    hr_keywords = [\n",
        "        \"hr team\", \"human resources team\", \"recruiting team\", \"talent team\",\n",
        "        \"head of hr\", \"hr manager\", \"hr department\", \"people operations\"\n",
        "    ]\n",
        "    return any(keyword in company_text.lower() for keyword in hr_keywords)\n",
        "\n",
        "def has_uk_presence(company_text):\n",
        "    \"\"\"Detect if company has UK presence\"\"\"\n",
        "    uk_keywords = [\n",
        "        \"uk office\", \"united kingdom\", \"london office\", \"manchester\", \"birmingham\",\n",
        "        \"uk team\", \"uk based\", \"offices in the uk\", \"uk headquarters\"\n",
        "    ]\n",
        "    return any(keyword in company_text.lower() for keyword in uk_keywords)\n",
        "\n",
        "def has_ecommerce(company_text):\n",
        "    \"\"\"Detect if company has an ecommerce store\"\"\"\n",
        "    ecommerce_keywords = [\n",
        "        \"shop now\", \"add to cart\", \"buy online\", \"online store\", \"ecommerce\",\n",
        "        \"shopping cart\", \"checkout\", \"product page\", \"online shop\"\n",
        "    ]\n",
        "    return any(keyword in company_text.lower() for keyword in ecommerce_keywords)\n",
        "\n",
        "def mentions_ai(company_text):\n",
        "    \"\"\"Detect if company mentions AI\"\"\"\n",
        "    ai_keywords = [\n",
        "        \"artificial intelligence\", \"machine learning\", \"deep learning\", \"ai \",\n",
        "        \"neural network\", \"natural language processing\", \"nlp\", \"computer vision\"\n",
        "    ]\n",
        "    return any(keyword in company_text.lower() for keyword in ai_keywords)\n",
        "\n",
        "# Main enrichment function\n",
        "def enrich_company_data(df, progress_bar):\n",
        "    \"\"\"Enrich company data with web research\"\"\"\n",
        "    enriched_df = df.copy()\n",
        "\n",
        "    # Add enrichment columns if they don't exist\n",
        "    new_columns = [\n",
        "        'detailed_industry', 'has_sales_jobs', 'has_hr_team',\n",
        "        'has_uk_presence', 'ecommerce_store', 'ai_mentions', 'data_confidence'\n",
        "    ]\n",
        "\n",
        "    for col in new_columns:\n",
        "        if col not in enriched_df.columns:\n",
        "            enriched_df[col] = None\n",
        "\n",
        "    # Process each company\n",
        "    for idx, row in enriched_df.iterrows():\n",
        "        company_name = row['company_name']\n",
        "\n",
        "        # Get domain from different possible column names\n",
        "        domain = None\n",
        "        for domain_col in ['company_domain', 'company_website', 'website', 'domain']:\n",
        "            if domain_col in row and pd.notna(row[domain_col]):\n",
        "                domain = row[domain_col]\n",
        "                # Ensure domain has http/https prefix\n",
        "                if domain and not domain.startswith(('http://', 'https://')):\n",
        "                    domain = 'https://' + domain\n",
        "                break\n",
        "\n",
        "        # Update progress bar text and value\n",
        "        progress_text = f\"Processing {company_name}\"\n",
        "        progress_bar.progress((idx + 1) / len(enriched_df), text=progress_text)\n",
        "\n",
        "        # Search for company information\n",
        "        search_results = search_company(company_name, domain)\n",
        "\n",
        "        # Aggregate text from search results\n",
        "        all_text = \"\"\n",
        "        confidence = 0\n",
        "\n",
        "        # If we have a domain, try to extract info directly from company website\n",
        "        website_info = {}\n",
        "        if domain:\n",
        "            website_info = extract_website_info(domain)\n",
        "            if 'error' not in website_info:\n",
        "                all_text += website_info.get('meta_description', '') + \" \"\n",
        "                all_text += website_info.get('about_section', '') + \" \"\n",
        "                all_text += website_info.get('full_text', '')\n",
        "                confidence += 1\n",
        "\n",
        "        # Process search results\n",
        "        for i, result in enumerate(search_results):\n",
        "            if isinstance(result, dict) and 'body' in result:\n",
        "                all_text += result['body'] + \" \"\n",
        "                confidence += 0.5\n",
        "\n",
        "                # Try to extract more details from the first few results\n",
        "                if i < 2 and 'href' in result:\n",
        "                    site_info = extract_website_info(result['href'])\n",
        "                    if 'error' not in site_info:\n",
        "                        all_text += site_info.get('full_text', '')\n",
        "                        confidence += 0.5\n",
        "\n",
        "        # Perform analysis\n",
        "        if all_text:\n",
        "            # Classify industry\n",
        "            detailed_industry = classify_industry(all_text)\n",
        "            enriched_df.at[idx, 'detailed_industry'] = detailed_industry\n",
        "\n",
        "            # Detect various indicators\n",
        "            enriched_df.at[idx, 'has_sales_jobs'] = has_sales_jobs(all_text)\n",
        "            enriched_df.at[idx, 'has_hr_team'] = has_hr_team(all_text)\n",
        "            enriched_df.at[idx, 'has_uk_presence'] = has_uk_presence(all_text)\n",
        "            enriched_df.at[idx, 'ecommerce_store'] = has_ecommerce(all_text)\n",
        "            enriched_df.at[idx, 'ai_mentions'] = mentions_ai(all_text)\n",
        "\n",
        "            # Set confidence score (0-5)\n",
        "            enriched_df.at[idx, 'data_confidence'] = min(confidence, 5)\n",
        "        else:\n",
        "            enriched_df.at[idx, 'data_confidence'] = 0\n",
        "\n",
        "        # Avoid rate limiting\n",
        "        time.sleep(search_delay)\n",
        "\n",
        "    # Complete the progress bar\n",
        "    progress_bar.progress(1.0, text=\"Processing complete!\")\n",
        "    return enriched_df\n",
        "\n",
        "# Visualization Functions\n",
        "def create_visualizations(df):\n",
        "    \"\"\"Create various visualizations of the enriched data\"\"\"\n",
        "\n",
        "    # Create columns for layout\n",
        "    col1, col2 = st.columns(2)\n",
        "\n",
        "    with col1:\n",
        "        # Industry Distribution\n",
        "        st.subheader(\"Industry Distribution\")\n",
        "        if 'detailed_industry' in df.columns:\n",
        "            industry_counts = df['detailed_industry'].value_counts()\n",
        "            fig, ax = plt.subplots(figsize=(10, 6))\n",
        "            sns.barplot(x=industry_counts.index, y=industry_counts.values, ax=ax)\n",
        "            plt.title('Company Distribution by Industry')\n",
        "            plt.xlabel('Industry')\n",
        "            plt.ylabel('Number of Companies')\n",
        "            plt.xticks(rotation=45, ha='right')\n",
        "            plt.tight_layout()\n",
        "            st.pyplot(fig)\n",
        "        else:\n",
        "            st.info(\"Industry data not available.\")\n",
        "\n",
        "        # Data Confidence\n",
        "        st.subheader(\"Data Confidence Distribution\")\n",
        "        if 'data_confidence' in df.columns:\n",
        "            fig, ax = plt.subplots(figsize=(10, 6))\n",
        "            sns.histplot(df['data_confidence'], bins=5, kde=True, ax=ax)\n",
        "            plt.title('Data Confidence Distribution')\n",
        "            plt.xlabel('Confidence Score (0-5)')\n",
        "            plt.ylabel('Number of Companies')\n",
        "            plt.tight_layout()\n",
        "            st.pyplot(fig)\n",
        "        else:\n",
        "            st.info(\"Confidence data not available.\")\n",
        "\n",
        "    with col2:\n",
        "        # Binary Indicators\n",
        "        st.subheader(\"Company Indicators\")\n",
        "        indicators = ['has_sales_jobs', 'has_hr_team', 'has_uk_presence', 'ecommerce_store', 'ai_mentions']\n",
        "        valid_indicators = [col for col in indicators if col in df.columns]\n",
        "\n",
        "        if valid_indicators:\n",
        "            counts = {}\n",
        "            for indicator in valid_indicators:\n",
        "                true_count = df[indicator].sum()\n",
        "                false_count = len(df) - true_count\n",
        "                counts[indicator] = [true_count, false_count]\n",
        "\n",
        "            fig, ax = plt.subplots(figsize=(10, 8))\n",
        "            x = np.arange(len(counts))\n",
        "            width = 0.35\n",
        "\n",
        "            # Plot\n",
        "            ax.bar(x - width/2, [counts[ind][0] for ind in counts], width, label='Yes')\n",
        "            ax.bar(x + width/2, [counts[ind][1] for ind in counts], width, label='No')\n",
        "\n",
        "            ax.set_xlabel('Indicator')\n",
        "            ax.set_ylabel('Number of Companies')\n",
        "            ax.set_title('Company Indicators Distribution')\n",
        "            ax.set_xticks(x)\n",
        "            ax.set_xticklabels([ind.replace('_', ' ').replace('has ', '').title() for ind in counts.keys()])\n",
        "            plt.xticks(rotation=45, ha='right')\n",
        "            ax.legend()\n",
        "            plt.tight_layout()\n",
        "            st.pyplot(fig)\n",
        "        else:\n",
        "            st.info(\"Indicator data not available.\")\n",
        "\n",
        "        # Summary metrics\n",
        "        st.subheader(\"Summary Metrics\")\n",
        "        metrics_col1, metrics_col2 = st.columns(2)\n",
        "\n",
        "        # Calculate metrics\n",
        "        total_companies = len(df)\n",
        "        companies_with_sales = df['has_sales_jobs'].sum() if 'has_sales_jobs' in df.columns else 0\n",
        "        companies_with_hr = df['has_hr_team'].sum() if 'has_hr_team' in df.columns else 0\n",
        "        companies_with_ecommerce = df['ecommerce_store'].sum() if 'ecommerce_store' in df.columns else 0\n",
        "        companies_with_ai = df['ai_mentions'].sum() if 'ai_mentions' in df.columns else 0\n",
        "\n",
        "        with metrics_col1:\n",
        "            st.metric(\"Total Companies\", total_companies)\n",
        "            st.metric(\"Companies with Sales Jobs\", f\"{companies_with_sales} ({int(companies_with_sales/total_companies*100 if total_companies else 0)}%)\")\n",
        "            st.metric(\"Companies with HR Team\", f\"{companies_with_hr} ({int(companies_with_hr/total_companies*100 if total_companies else 0)}%)\")\n",
        "\n",
        "        with metrics_col2:\n",
        "            st.metric(\"Companies with Ecommerce\", f\"{companies_with_ecommerce} ({int(companies_with_ecommerce/total_companies*100 if total_companies else 0)}%)\")\n",
        "            st.metric(\"Companies mentioning AI\", f\"{companies_with_ai} ({int(companies_with_ai/total_companies*100 if total_companies else 0)}%)\")\n",
        "\n",
        "# Main application flow\n",
        "def main():\n",
        "    # Create tabs\n",
        "    tab1, tab2, tab3 = st.tabs([\"Upload & Process\", \"Results & Visualization\", \"Export\"])\n",
        "\n",
        "    # Tab 1: Upload and Process\n",
        "    with tab1:\n",
        "        st.header(\"Upload Company Data\")\n",
        "\n",
        "        # File uploader\n",
        "        uploaded_file = st.file_uploader(\"Choose a CSV file with company data\", type=['csv', 'xlsx'])\n",
        "\n",
        "        if uploaded_file is not None:\n",
        "            # Read the data\n",
        "            try:\n",
        "                if uploaded_file.name.endswith('.csv'):\n",
        "                    df = pd.read_csv(uploaded_file)\n",
        "                else:\n",
        "                    df = pd.read_excel(uploaded_file)\n",
        "\n",
        "                # Check required columns\n",
        "                if 'company_name' not in df.columns:\n",
        "                    st.error(\"The uploaded file must contain a 'company_name' column.\")\n",
        "                    st.stop()\n",
        "\n",
        "                # Display original data\n",
        "                st.subheader(\"Original Data\")\n",
        "                st.dataframe(df)\n",
        "\n",
        "                # Store the dataframe in session state for access across tabs\n",
        "                st.session_state.original_df = df\n",
        "\n",
        "                # Process options\n",
        "                st.subheader(\"Processing Options\")\n",
        "\n",
        "                # Allow limiting the number of companies to process\n",
        "                process_all = st.checkbox(\"Process all companies\", value=df.shape[0] <= 10)\n",
        "\n",
        "                if not process_all:\n",
        "                    max_companies = st.slider(\"Number of companies to process\", 1, min(df.shape[0], 30), min(5, df.shape[0]))\n",
        "                else:\n",
        "                    max_companies = df.shape[0]\n",
        "\n",
        "                # Custom indicators\n",
        "                st.subheader(\"Custom Indicators (Coming Soon)\")\n",
        "                st.info(\"In a future version, you'll be able to define your own indicators here.\")\n",
        "\n",
        "                # Start processing button\n",
        "                if st.button(\"Start Enrichment Process\"):\n",
        "                    # Create a subset of data if needed\n",
        "                    process_df = df.head(max_companies).copy()\n",
        "\n",
        "                    # Set up a progress bar\n",
        "                    st.subheader(\"Processing Progress\")\n",
        "                    progress_bar = st.progress(0, text=\"Starting...\")\n",
        "\n",
        "                    # Process the data\n",
        "                    with st.spinner('Searching the web for company information...'):\n",
        "                        enriched_df = enrich_company_data(process_df, progress_bar)\n",
        "\n",
        "                    # Store enriched data in session state\n",
        "                    st.session_state.enriched_df = enriched_df\n",
        "\n",
        "                    # Success message\n",
        "                    st.success(f\"‚úÖ Successfully processed {len(enriched_df)} companies!\")\n",
        "\n",
        "                    # Prompt to view results\n",
        "                    st.info(\"Click on the 'Results & Visualization' tab to view the enriched data.\")\n",
        "\n",
        "            except Exception as e:\n",
        "                st.error(f\"Error processing file: {e}\")\n",
        "                st.exception(e)\n",
        "\n",
        "        else:\n",
        "            # Show sample data option if no file uploaded\n",
        "            if st.button(\"Use sample data\"):\n",
        "                # Create sample data\n",
        "                sample_data = {\n",
        "                    'company_name': ['TechNova Solutions', 'Green Leaf Organics', 'MediHealth Systems',\n",
        "                                     'Global Travel Partners', 'EduLearn Academy'],\n",
        "                    'company_domain': ['technova.com', 'greenleaforganics.co.uk', 'medihealthsystems.org',\n",
        "                                      'globaltravelpartners.net', 'edulearn.edu'],\n",
        "                    'industry': ['IT Services', 'Agriculture', 'Healthcare', 'Travel', 'Education'],\n",
        "                    'location': ['San Francisco', 'London', 'Boston', 'New York', 'Chicago']\n",
        "                }\n",
        "\n",
        "                sample_df = pd.DataFrame(sample_data)\n",
        "                st.session_state.original_df = sample_df\n",
        "\n",
        "                # Display sample data\n",
        "                st.subheader(\"Sample Data\")\n",
        "                st.dataframe(sample_df)\n",
        "\n",
        "                # Show processing button\n",
        "                if st.button(\"Process Sample Data\"):\n",
        "                    # Set up a progress bar\n",
        "                    st.subheader(\"Processing Progress\")\n",
        "                    progress_bar = st.progress(0, text=\"Starting...\")\n",
        "\n",
        "                    # Process the data\n",
        "                    with st.spinner('Searching the web for company information...'):\n",
        "                        enriched_df = enrich_company_data(sample_df, progress_bar)\n",
        "\n",
        "                    # Store enriched data in session state\n",
        "                    st.session_state.enriched_df = enriched_df\n",
        "\n",
        "                    # Success message\n",
        "                    st.success(f\"‚úÖ Successfully processed {len(enriched_df)} companies!\")\n",
        "\n",
        "                    # Prompt to view results\n",
        "                    st.info(\"Click on the 'Results & Visualization' tab to view the enriched data.\")\n",
        "\n",
        "    # Tab 2: Results and Visualization\n",
        "    with tab2:\n",
        "        st.header(\"Enriched Data Results\")\n",
        "\n",
        "        if 'enriched_df' in st.session_state:\n",
        "            # Display enriched data\n",
        "            st.subheader(\"Enriched Company Data\")\n",
        "            st.dataframe(st.session_state.enriched_df)\n",
        "\n",
        "            # Create visualizations\n",
        "            create_visualizations(st.session_state.enriched_df)\n",
        "        else:\n",
        "            st.info(\"No enriched data available yet. Please upload and process data in the first tab.\")\n",
        "\n",
        "    # Tab 3: Export\n",
        "    with tab3:\n",
        "        st.header(\"Export Enriched Data\")\n",
        "\n",
        "        if 'enriched_df' in st.session_state:\n",
        "            # Add timestamp to filename\n",
        "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            filename = f\"enriched_company_data_{timestamp}.csv\"\n",
        "\n",
        "            # Convert dataframe to CSV\n",
        "            csv = st.session_state.enriched_df.to_csv(index=False)\n",
        "\n",
        "            # Download button\n",
        "            st.download_button(\n",
        "                label=\"Download Enriched Data as CSV\",\n",
        "                data=csv,\n",
        "                file_name=filename,\n",
        "                mime=\"text/csv\",\n",
        "                key=\"download-csv\"\n",
        "            )\n",
        "\n",
        "            # Show export to CRM options (for future implementation)\n",
        "            st.subheader(\"Export to CRM (Coming Soon)\")\n",
        "\n",
        "            # Create columns for CRM options\n",
        "            crm_col1, crm_col2, crm_col3 = st.columns(3)\n",
        "\n",
        "            with crm_col1:\n",
        "                st.button(\"Export to Salesforce\", disabled=True)\n",
        "\n",
        "            with crm_col2:\n",
        "                st.button(\"Export to HubSpot\", disabled=True)\n",
        "\n",
        "            with crm_col3:\n",
        "                st.button(\"Export to Custom API\", disabled=True)\n",
        "\n",
        "            st.info(\"CRM integration will be available in a future update.\")\n",
        "\n",
        "        else:\n",
        "            st.info(\"No enriched data available yet. Please upload and process data in the first tab.\")\n",
        "\n",
        "# Run the app\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qdlVOeYNOwK2",
        "outputId": "42e84b55-21aa-4bc2-b807-eaf495fafd04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok authtoken 2ib50v1FOA6wAqxOTCfZoo9FlMs_6z9Xpydw78W9yFkYy7jSN"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2iYeb5ePsN4",
        "outputId": "b41d304c-1f16-4d29-bdbb-cad78ea08a7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "!streamlit run app.py &>/dev/null&\n",
        "ngrok_tunnel = ngrok.connect(8501)\n",
        "print(f\"Streamlit is running at: {ngrok_tunnel.public_url}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BAJQT3COyG9",
        "outputId": "727b5701-22db-4062-898f-807e05eb9555"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streamlit is running at: https://f21d-35-194-253-178.ngrok-free.app\n"
          ]
        }
      ]
    }
  ]
}